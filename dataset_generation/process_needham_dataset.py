"""
Process the Needham dataset to extract and format conversation samples.

This script processes raw conversation data from the Needham dataset and transforms
it into a standardized format suitable for evaluation. It:
- Formats messages for OpenAI API compatibility
- Computes eval_human scores from multiple author ratings
- Filters and cleans metadata fields
"""

import json
from typing import Any, Dict, List


def process_message(message: Dict[str, Any]) -> Dict[str, Any]:
    """
    Process a single message to format it for OpenAI API.
    Removes the 'id' field and keeps only relevant fields.
    Converts tool_calls to the correct OpenAI API format.
    """
    processed = {
        "role": message["role"],
        "content": message["content"]
    }
    
    # Convert tool_calls to OpenAI API format if present
    if "tool_calls" in message:
        tool_calls = []
        for tc in message["tool_calls"]:
            # Handle both old and new formats
            if "type" in tc and "function" in tc and isinstance(tc["function"], dict):
                # Already in correct format
                tool_calls.append(tc)
            else:
                # Convert from old format to OpenAI format
                # Old format: {"id": "...", "function": "name", "arguments": {...}}
                # New format: {"id": "...", "type": "function", "function": {"name": "...", "arguments": "..."}}
                tool_call = {
                    "id": tc["id"],
                    "type": "function",
                    "function": {
                        "name": tc["function"],
                        "arguments": json.dumps(tc["arguments"])
                    }
                }
                tool_calls.append(tool_call)
        
        processed["tool_calls"] = tool_calls
    
    # Keep tool_call_id if present (for tool response messages)
    if "tool_call_id" in message:
        processed["tool_call_id"] = message["tool_call_id"]
    
    return processed


def process_metadata(metadata: Dict[str, Any]) -> Dict[str, Any]:
    """
    Process metadata to keep only specified fields and compute eval_human.
    """
    eval_author1 = metadata.get("eval_author1", 0)
    eval_author2 = metadata.get("eval_author2", 0)
    eval_human = round((eval_author1 + eval_author2) / 2, 4)
    
    processed_metadata = {
        "category": metadata.get("category"),
        "eval_category": metadata.get("eval_category"),
        "safety_category": metadata.get("safety_category"),
        "tools": metadata.get("tools"),
        "eval_type": metadata.get("eval_type"),
        "eval_human": eval_human
    }
    
    return processed_metadata


def should_include_sample(sample_id: str) -> bool:
    """
    Determine if a sample should be included in the processed dataset.
    
    Excludes:
    - All samples starting with "aider_commit"
    - All samples starting with "irl_cursor"
    - All samples starting with "continue_dev" except for specific IDs
    """
    # Exclude aider_commit samples
    if sample_id.startswith("aider_commit"):
        return False
    
    # Exclude irl_cursor samples
    if sample_id.startswith("irl_cursor"):
        return False
    
    # For continue_dev, only include specific samples
    if sample_id.startswith("continue_dev"):
        allowed_continue_dev = ["continue_dev:2", "continue_dev:8", "continue_dev:13", "continue_dev:22"]
        return sample_id in allowed_continue_dev
    
    # Include all other samples
    return True


def process_sample(sample: Dict[str, Any]) -> Dict[str, Any]:
    """
    Process a single sample from the dataset.
    """
    processed_sample = {
        "id": sample["id"],
        "input": [process_message(msg) for msg in sample["input"]],
        "metadata": process_metadata(sample["metadata"])
    }
    
    return processed_sample


def main():
    input_file = "../datasets/needham_dataset.json"
    output_file = "../datasets/adapted_needham_dataset.json"
    
    print("Reading dataset...")
    with open(input_file, 'r', encoding='utf-8') as f:
        dataset = json.load(f)
    
    print(f"Filtering and processing {len(dataset)} samples...")
    # Filter samples based on ID criteria
    filtered_dataset = [sample for sample in dataset if should_include_sample(sample["id"])]
    print(f"Samples after filtering: {len(filtered_dataset)} (excluded {len(dataset) - len(filtered_dataset)} samples)")
    
    # Process the filtered samples
    processed_dataset = [process_sample(sample) for sample in filtered_dataset]
    
    print("Writing processed dataset...")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(processed_dataset, f, indent=2, ensure_ascii=False)
    
    print(f"Done! Processed dataset saved to: {output_file}")
    print(f"Total samples processed: {len(processed_dataset)}")


if __name__ == "__main__":
    main()

